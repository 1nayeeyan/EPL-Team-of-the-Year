{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "366d142c",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af5c90b",
   "metadata": {},
   "source": [
    "I will create webscraping scripts to collect and aggregate player data, mainly from this website https://fbref.com/en/comps/9/Premier-League-Stats\n",
    "\n",
    "General Player Stats\n",
    "https://fbref.com/en/comps/9/stats/Premier-League-Stats#all_stats_standard\n",
    "\n",
    "Goalkeeping Stats\n",
    "https://fbref.com/en/comps/9/keepers/Premier-League-Stats\n",
    "\n",
    "Passing Stats\n",
    "https://fbref.com/en/comps/9/passing/Premier-League-Stats\n",
    "\n",
    "Defensive Stats\n",
    "https://fbref.com/en/comps/9/defense/Premier-League-Stats\n",
    "\n",
    "Shooting Stats\n",
    "https://fbref.com/en/comps/9/shooting/Premier-League-Stats\n",
    "\n",
    "My plan is to create 4 different sets of data. One for goalkeepers, the next for defensive stats, then midfielders and finally attacking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76394848",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da89f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da18d1e1",
   "metadata": {},
   "source": [
    "##  Scrapping Goalie Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d180a76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since the page dynamically loads, I am going to use Selenium\n",
    "\n",
    "# Set up Selenium options\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "\n",
    "# Set path to chromedriver executable\n",
    "# Replace 'path/to/chromedriver' with the actual path to your chromedriver\n",
    "driver_path = 'path/to/chromedriver'\n",
    "\n",
    "# Set up the Selenium service\n",
    "service = Service(driver_path)\n",
    "\n",
    "# Set up the WebDriver instance\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Navigate to the webpage\n",
    "url = \"https://fbref.com/en/comps/9/keepers/Premier-League-Stats\"\n",
    "driver.get(url)\n",
    "\n",
    "# Find the parent div using Selenium\n",
    "div = driver.find_element(By.ID, \"div_stats_keeper\")\n",
    "\n",
    "# Get the HTML content of the div\n",
    "div_html = div.get_attribute(\"innerHTML\")\n",
    "\n",
    "# Close the WebDriver instance\n",
    "driver.quit()\n",
    "\n",
    "# Create a BeautifulSoup object from the div HTML\n",
    "soup = BeautifulSoup(div_html, 'html.parser')\n",
    "\n",
    "# Find the table within the div\n",
    "table = soup.find(\"table\")\n",
    "\n",
    "# Create empty lists to store the data\n",
    "player_data = []\n",
    "\n",
    "# Iterate over the rows in the table\n",
    "rows = table.find(\"tbody\").find_all(\"tr\")\n",
    "#print(rows)\n",
    "\n",
    "for row in rows:\n",
    "    \n",
    "    if \"thead\" in row.get(\"class\", []):\n",
    "        continue\n",
    "    \n",
    "    # Extract the data from each column in the row\n",
    "    columns = row.find_all(\"td\")\n",
    "    player_name = columns[0].text.strip()\n",
    "    position = columns[2].text.strip()\n",
    "    team = columns[3].text.strip()\n",
    "    saves = int(columns[13].text.strip())\n",
    "    save_percentage = float(columns[14].text.strip())\n",
    "    clean_sheet_percentage = float(columns[19].text.strip())\n",
    "    \n",
    "    # Append the data as a dictionary to the player_data list\n",
    "    player_data.append({\n",
    "        \"Player Name\": player_name,\n",
    "        \"Position\": position,\n",
    "        \"Team\": team,\n",
    "        \"Saves\": saves,\n",
    "        \"Save Percentage\": save_percentage,\n",
    "        \"Clean Sheet Percentage\": clean_sheet_percentage\n",
    "    })\n",
    "\n",
    "# Create a pandas DataFrame from the player_data list\n",
    "df = pd.DataFrame(player_data)\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "file_path = os.path.join(current_directory, \"goalkeeper_stats.csv\")\n",
    "df.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7013b0bd",
   "metadata": {},
   "source": [
    "## Scrapping Defender Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d34eb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Selenium options\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "\n",
    "# Set path to chromedriver executable\n",
    "# Replace 'path/to/chromedriver' with the actual path to your chromedriver\n",
    "driver_path = 'path/to/chromedriver'\n",
    "\n",
    "# Set up the Selenium service\n",
    "service = Service(driver_path)\n",
    "\n",
    "# Set up the WebDriver instance\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Navigate to the webpage\n",
    "url = \"https://fbref.com/en/comps/9/defense/Premier-League-Stats\"\n",
    "driver.get(url)\n",
    "\n",
    "# Find the parent div using Selenium\n",
    "div = driver.find_element(By.ID, \"div_stats_defense\")\n",
    "\n",
    "# Get the HTML content of the div\n",
    "div_html = div.get_attribute(\"innerHTML\")\n",
    "\n",
    "# Close the WebDriver instance\n",
    "driver.quit()\n",
    "\n",
    "# Create a BeautifulSoup object from the div HTML\n",
    "soup = BeautifulSoup(div_html, 'html.parser')\n",
    "\n",
    "# Find the table within the div\n",
    "table = soup.find(\"table\")\n",
    "\n",
    "# Create empty lists to store the data\n",
    "player_data = []\n",
    "\n",
    "# Iterate over the rows in the table\n",
    "rows = table.find(\"tbody\").find_all(\"tr\")\n",
    "#print(rows)\n",
    "\n",
    "for row in rows:\n",
    "    \n",
    "    if \"thead\" in row.get(\"class\", []):\n",
    "        continue\n",
    "    \n",
    "    # Extract the data from each column in the row\n",
    "    columns = row.find_all(\"td\")\n",
    "    player_name = columns[0].text.strip()\n",
    "    position = columns[2].text.strip()\n",
    "    team = columns[3].text.strip()\n",
    "    tackles_won = int(columns[8].text.strip())\n",
    "    shots_blocked = int(columns[17].text.strip())\n",
    "    interceptions = int(columns[19].text.strip())\n",
    "    clearances = int(columns[21].text.strip())\n",
    "    \n",
    "    \n",
    "    # Append the data as a dictionary to the player_data list\n",
    "    player_data.append({\n",
    "        \"Player Name\": player_name,\n",
    "        \"Position\": position,\n",
    "        \"Team\": team,\n",
    "        \"Tackles Won\": tackles_won,\n",
    "        \"Shots Blocked\": shots_blocked,\n",
    "        \"Interceptions\": interceptions,\n",
    "        \"Clearances\": clearances\n",
    "    })\n",
    "\n",
    "# Create a pandas DataFrame from the player_data list\n",
    "df = pd.DataFrame(player_data)\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "file_path = os.path.join(current_directory, \"defensive_stats.csv\")\n",
    "df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe912bf",
   "metadata": {},
   "source": [
    "## Scrapping Midfielder Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "32aaa2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Selenium options\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "\n",
    "# Set path to chromedriver executable\n",
    "# Replace 'path/to/chromedriver' with the actual path to your chromedriver\n",
    "driver_path = 'path/to/chromedriver'\n",
    "\n",
    "# Set up the Selenium service\n",
    "service = Service(driver_path)\n",
    "\n",
    "# Set up the WebDriver instance\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Navigate to the webpage\n",
    "url = \"https://fbref.com/en/comps/9/passing/Premier-League-Stats\"\n",
    "driver.get(url)\n",
    "\n",
    "# Find the parent div using Selenium\n",
    "div = driver.find_element(By.ID, \"div_stats_passing\")\n",
    "\n",
    "# Get the HTML content of the div\n",
    "div_html = div.get_attribute(\"innerHTML\")\n",
    "\n",
    "# Create a BeautifulSoup object from the div HTML\n",
    "soup = BeautifulSoup(div_html, 'html.parser')\n",
    "\n",
    "# Find the table within the div\n",
    "table = soup.find(\"table\")\n",
    "\n",
    "# Create empty lists to store the data\n",
    "player_data = []\n",
    "\n",
    "# Iterate over the rows in the table\n",
    "rows = table.find(\"tbody\").find_all(\"tr\")\n",
    "#print(rows)\n",
    "\n",
    "for row in rows:\n",
    "    \n",
    "    if \"thead\" in row.get(\"class\", []):\n",
    "        continue\n",
    "    \n",
    "    # Extract the data from each column in the row\n",
    "    columns = row.find_all(\"td\")\n",
    "    player_name = columns[0].text.strip()\n",
    "    position = columns[2].text.strip()\n",
    "    team = columns[3].text.strip()\n",
    "    passes = int(columns[7].text.strip())\n",
    "    pass_completion = str(columns[9].text.strip())\n",
    "    assists = int(columns[21].text.strip())\n",
    "    key_passes = int(columns[25].text.strip())\n",
    "    \n",
    "    \n",
    "    # Append the data as a dictionary to the player_data list\n",
    "    player_data.append({\n",
    "        \"Player Name\": player_name,\n",
    "        \"Position\": position,\n",
    "        \"Team\": team,\n",
    "        \"Passes\": passes,\n",
    "        \"Pass Completion Rate\": pass_completion,\n",
    "        \"Assists\": assists,\n",
    "        \"Key Passses\": key_passes\n",
    "    })\n",
    "\n",
    "#from a table on another index of the site, I have to get the successul dribbles of each player\n",
    "\n",
    "# Scrape successful dribbles\n",
    "dribbles_url = \"https://fbref.com/en/comps/9/possession/Premier-League-Stats\"\n",
    "driver.get(dribbles_url)\n",
    "\n",
    "dribbles_data = []\n",
    "\n",
    "# Find the parent div using Selenium\n",
    "dribbles_div = driver.find_element(By.ID, \"div_stats_possession\")\n",
    "\n",
    "# Get the HTML content of the div\n",
    "dribbles_div_html = dribbles_div.get_attribute(\"innerHTML\")\n",
    "\n",
    "# Create a BeautifulSoup object from the div HTML\n",
    "dribbles_soup = BeautifulSoup(dribbles_div_html, 'html.parser')\n",
    "\n",
    "# Find the table within the div\n",
    "dribbles_table = dribbles_soup.find(\"table\")\n",
    "\n",
    "# Iterate over the rows in the table\n",
    "dribbles_rows = dribbles_table.find(\"tbody\").find_all(\"tr\")\n",
    "\n",
    "for row in dribbles_rows:\n",
    "    if \"thead\" in row.get(\"class\", []):\n",
    "        continue\n",
    "    \n",
    "    # Extract the data from each column in the row\n",
    "    columns = row.find_all(\"td\")\n",
    "    player_name = columns[0].text.strip()\n",
    "    successful_dribbles = int(columns[15].text.strip())\n",
    "    \n",
    "    # Append the data as a dictionary to the dribbles_data list\n",
    "    dribbles_data.append({\n",
    "        \"Player Name\": player_name,\n",
    "        \"Successful Dribbles\": successful_dribbles\n",
    "    })\n",
    "\n",
    "# Close the WebDriver instance\n",
    "driver.quit()\n",
    "\n",
    "# Create pandas DataFrames from the data lists\n",
    "passing_df = pd.DataFrame(player_data)\n",
    "dribbles_df = pd.DataFrame(dribbles_data)\n",
    "\n",
    "# Merge the passing and dribbles DataFrames based on player name\n",
    "merged_df = passing_df.merge(dribbles_df, on=\"Player Name\", how=\"left\")\n",
    "\n",
    "# Save the merged DataFrame to the passing_stats.csv file\n",
    "file_path = \"passing_stats.csv\"\n",
    "merged_df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bc0410",
   "metadata": {},
   "source": [
    "## Scrapping Attacking Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e00ba244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Selenium options\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "\n",
    "# Set path to chromedriver executable\n",
    "# Replace 'path/to/chromedriver' with the actual path to your chromedriver\n",
    "driver_path = 'path/to/chromedriver'\n",
    "\n",
    "# Set up the Selenium service\n",
    "service = Service(driver_path)\n",
    "\n",
    "# Set up the WebDriver instance\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Navigate to the webpage\n",
    "url = \"https://fbref.com/en/comps/9/shooting/Premier-League-Stats\"\n",
    "driver.get(url)\n",
    "\n",
    "# Find the parent div using Selenium\n",
    "div = driver.find_element(By.ID, \"div_stats_shooting\")\n",
    "\n",
    "# Get the HTML content of the div\n",
    "div_html = div.get_attribute(\"innerHTML\")\n",
    "\n",
    "# Close the WebDriver instance\n",
    "driver.quit()\n",
    "\n",
    "# Create a BeautifulSoup object from the div HTML\n",
    "soup = BeautifulSoup(div_html, 'html.parser')\n",
    "\n",
    "# Find the table within the div\n",
    "table = soup.find(\"table\")\n",
    "\n",
    "# Create empty lists to store the data\n",
    "player_data = []\n",
    "\n",
    "# Iterate over the rows in the table\n",
    "rows = table.find(\"tbody\").find_all(\"tr\")\n",
    "#print(rows)\n",
    "\n",
    "for row in rows:\n",
    "    \n",
    "    if \"thead\" in row.get(\"class\", []):\n",
    "        continue\n",
    "    \n",
    "    # Extract the data from each column in the row\n",
    "    columns = row.find_all(\"td\")\n",
    "    player_name = columns[0].text.strip()\n",
    "    position = columns[2].text.strip()\n",
    "    team = columns[3].text.strip()\n",
    "    goals = int(columns[7].text.strip())\n",
    "    shots_on_target = float(columns[9].text.strip())\n",
    "    goals_per_shot = str(columns[13].text.strip())\n",
    "    \n",
    "    \n",
    "    # Append the data as a dictionary to the player_data list\n",
    "    player_data.append({\n",
    "        \"Player Name\": player_name,\n",
    "        \"Position\": position,\n",
    "        \"Team\": team,\n",
    "        \"Goals\": goals,\n",
    "        \"Shots On Target\": shots_on_target,\n",
    "        \"Goals Per Shot\": goals_per_shot,\n",
    "    })\n",
    "\n",
    "# Create a pandas DataFrame from the player_data list\n",
    "df = pd.DataFrame(player_data)\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "file_path = os.path.join(current_directory, \"forward_stats.csv\")\n",
    "df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfad5eb9",
   "metadata": {},
   "source": [
    "## General Player Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2db760bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Selenium options\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "\n",
    "# Set path to chromedriver executable\n",
    "# Replace 'path/to/chromedriver' with the actual path to your chromedriver\n",
    "driver_path = 'path/to/chromedriver'\n",
    "\n",
    "# Set up the Selenium service\n",
    "service = Service(driver_path)\n",
    "\n",
    "# Set up the WebDriver instance\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Navigate to the webpage\n",
    "url = \"https://fbref.com/en/comps/9/stats/Premier-League-Stats\"\n",
    "driver.get(url)\n",
    "\n",
    "# Find the parent div using Selenium\n",
    "div = driver.find_element(By.ID, \"div_stats_standard\")\n",
    "\n",
    "# Get the HTML content of the div\n",
    "div_html = div.get_attribute(\"innerHTML\")\n",
    "\n",
    "# Close the WebDriver instance\n",
    "driver.quit()\n",
    "\n",
    "# Create a BeautifulSoup object from the div HTML\n",
    "soup = BeautifulSoup(div_html, 'html.parser')\n",
    "\n",
    "# Find the table within the div\n",
    "table = soup.find(\"table\")\n",
    "\n",
    "# Create empty lists to store the data\n",
    "player_data = []\n",
    "\n",
    "# Iterate over the rows in the table\n",
    "rows = table.find(\"tbody\").find_all(\"tr\")\n",
    "#print(rows)\n",
    "\n",
    "for row in rows:\n",
    "    \n",
    "    if \"thead\" in row.get(\"class\", []):\n",
    "        continue\n",
    "    \n",
    "    # Extract the data from each column in the row\n",
    "    columns = row.find_all(\"td\")\n",
    "    player_name = columns[0].text.strip()\n",
    "    position = columns[2].text.strip()\n",
    "    team = columns[3].text.strip()\n",
    "    appearances = int(columns[6].text.strip())\n",
    "    minutes_played = str(columns[8].text.strip())\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Append the data as a dictionary to the player_data list\n",
    "    player_data.append({\n",
    "        \"Player Name\": player_name,\n",
    "        \"Position\": position,\n",
    "        \"Team\": team,\n",
    "        \"Appearances\": appearances,\n",
    "        \"Minutes Played\": minutes_played,\n",
    "    })\n",
    "\n",
    "# Create a pandas DataFrame from the player_data list\n",
    "df = pd.DataFrame(player_data)\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "file_path = os.path.join(current_directory, \"general_stats.csv\")\n",
    "df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750930db",
   "metadata": {},
   "source": [
    "## Cleaning our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5404af52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "file_path = cwd+\"/goalkeeper_stats.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Fill missing values with zeros\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Save the cleaned dataframe to a new CSV file\n",
    "cleaned_file_path = cwd+\"/goalkeeper_stats.csv\"\n",
    "df.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "# Read the CSV file\n",
    "file_path = cwd+\"/defensive_stats.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Fill missing values with zeros\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Save the cleaned dataframe to a new CSV file\n",
    "cleaned_file_path = cwd+\"/defensive_stats.csv\"\n",
    "df.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "# Read the CSV file\n",
    "file_path = cwd+\"/passing_stats.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Fill missing values with zeros\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Save the cleaned dataframe to a new CSV file\n",
    "cleaned_file_path = cwd+\"/passing_stats.csv\"\n",
    "df.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "# Read the CSV file\n",
    "file_path = cwd+\"/forward_stats.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Fill missing values with zeros\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Save the cleaned dataframe to a new CSV file\n",
    "cleaned_file_path = cwd+\"/forward_stats.csv\"\n",
    "df.to_csv(cleaned_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c43c430",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
